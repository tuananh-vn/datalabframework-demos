{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engines:\n",
      "  spark:\n",
      "    config:\n",
      "      jars:\n",
      "      - http://www.datanucleus.org/downloads/maven2/oracle/ojdbc6/11.2.0.3/ojdbc6-11.2.0.3.jar\n",
      "      jobname: default\n",
      "      master: spark://spark-master:7077\n",
      "    context: spark\n",
      "loggers:\n",
      "  stream:\n",
      "    enable: true\n",
      "    severity: info\n",
      "profile: default\n",
      "providers:\n",
      "  ingest:\n",
      "    format: parquet\n",
      "    hostname: hdfs-nn\n",
      "    path: /data/ingest\n",
      "    service: hdfs\n",
      "    write:\n",
      "      coalesce: 2\n",
      "      options:\n",
      "        mode: append\n",
      "      repartition: 4\n",
      "  source:\n",
      "    database: system\n",
      "    hostname: oracle\n",
      "    password: oracle\n",
      "    port: 1521\n",
      "    read:\n",
      "      cache: true\n",
      "      repartition: 4\n",
      "    service: oracle\n",
      "    sid: xe\n",
      "    username: system\n",
      "resources:\n",
      "  .resources.actors:\n",
      "    filter:\n",
      "      column: last_update\n",
      "      policy: date\n",
      "    path: actors\n",
      "    provider: source\n",
      "variables: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dlf.utils.pretty_print(dlf.params.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS:  --jars http://www.datanucleus.org/downloads/maven2/oracle/ojdbc6/11.2.0.3/ojdbc6-11.2.0.3.jar pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORACLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provider:\n",
      "  database: system\n",
      "  hostname: oracle\n",
      "  password: oracle\n",
      "  path: ''\n",
      "  port: 1521\n",
      "  read:\n",
      "    cache: true\n",
      "    repartition: 4\n",
      "  service: oracle\n",
      "  sid: xe\n",
      "  username: system\n",
      "resource:\n",
      "  filter:\n",
      "    column: last_update\n",
      "    policy: date\n",
      "  path: actors\n",
      "  provider: source\n",
      "url: null\n",
      "\n",
      "jdbc:oracle:thin:system/oracle@//oracle:1521/xe\n",
      "system\n",
      "actors\n",
      "+--------+----------+---------+-----------+\n",
      "|ACTOR_ID|FIRST_NAME|LAST_NAME|LAST_UPDATE|\n",
      "+--------+----------+---------+-----------+\n",
      "+--------+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read by resource alias\n",
    "md = dlf.params.metadata()\n",
    "for resource in md['resources'].keys():\n",
    "    df_src = engine.read(resource)\n",
    "    df_src.rdd.getNumPartitions()\n",
    "    df_src.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
